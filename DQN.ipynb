{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmlLQaAri1W0"
      },
      "source": [
        "#DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkvHQT6djClS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque  #Deque is preferred over list in the cases where we need quicker append and pop operations\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eBBdU67m0rs"
      },
      "source": [
        "class replaybuffer():\n",
        "  def __init__(self,buffer_size):\n",
        "    self.buffer = deque(maxlen=buffer_size) #initilize deque that will store tuple\n",
        " \n",
        "  def add_to_buffer(self,state,action,reward,next_state,done): #add new elements in buffer\n",
        "    self.buffer.append({'state':np.array(state),\"action\":action,\"reward\":reward,\"next_state\":np.array(next_state),\"done\":done}) \n",
        " \n",
        "  def get_batch(self,batch_size): #return dictionary batch containing states,actions,rewards,next_states and dones tensor of specified batch size\n",
        "    sample = random.sample(self.buffer,batch_size)#random selections of elements from buffer\n",
        "    batch = {}\n",
        "    for key in sample[0].keys():\n",
        "      batch[key+'s']=torch.tensor([d[key] for d in sample])\n",
        "    return batch\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atQbhLrqT59h"
      },
      "source": [
        "try_buffer = replaybuffer(100)\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "  action = env.action_space.sample()\n",
        "  next_state,reward,done,_ = env.step(action)\n",
        "  try_buffer.add_to_buffer(state,action,reward,next_state,done)\n",
        "  state = next_state\n",
        "try_buffer.buffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIxgEVzKV2Hw",
        "outputId": "f8579c92-25a3-4a6d-9bb3-3b172d11b78f"
      },
      "source": [
        "batch = try_buffer.get_batch(5)\n",
        "batch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'actions': tensor([1, 1, 0, 0, 1]),\n",
              " 'dones': tensor([False, False, False, False, False]),\n",
              " 'next_states': tensor([[-0.0735, -0.0459,  0.0996,  0.0676],\n",
              "         [-0.0388, -0.4318,  0.0526,  0.5589],\n",
              "         [-0.0714, -0.0487,  0.0971,  0.1307],\n",
              "         [ 0.1362,  1.1096, -0.1327, -1.3613],\n",
              "         [ 0.1584,  1.3061, -0.1599, -1.6924]], dtype=torch.float64),\n",
              " 'rewards': tensor([1., 1., 1., 1., 1.]),\n",
              " 'states': tensor([[-0.0687, -0.2395,  0.0930,  0.3296],\n",
              "         [-0.0263, -0.6264,  0.0358,  0.8401],\n",
              "         [-0.0744,  0.1477,  0.1010, -0.1920],\n",
              "         [ 0.1102,  1.3034, -0.1003, -1.6211],\n",
              "         [ 0.1362,  1.1096, -0.1327, -1.3613]], dtype=torch.float64)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5XQ9DRYi4e_"
      },
      "source": [
        "class network(nn.Module):\n",
        "  def __init__(self,input_size,output_size):\n",
        "    super(network,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size,24) #First hidden layer with 24 neurons\n",
        "    self.fc2 = nn.Linear(24,24) #Second hidden layer with 24 neurons\n",
        "    self.output_layer = nn.Linear(24,output_size) \n",
        " \n",
        "  def forward(self,input):\n",
        "    out = F.relu(self.fc1(input))\n",
        "    out = F.relu(self.fc2(out))\n",
        "    out = self.output_layer(out)\n",
        " \n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHMJAu7U_jAt"
      },
      "source": [
        "class DQN_Agent():\n",
        "  def __init__(self,environment='CartPole-v0',learning_rate=0.001,start=0.9,end=0.005,decay=50,buffer_size = 10000):\n",
        "    self.eps_start=start     #epsilon at start of episode\n",
        "    self.eps_end=end         #epsilon at end of episode\n",
        "    self.eps_decay=decay     #epsilon decay rate\n",
        "    self.env = gym.make(environment)   \n",
        "    self.q_net = network(self.env.observation_space.shape[0],self.env.action_space.n)  #initilize q network \n",
        "    self.target_net = copy.deepcopy(self.q_net)  # intilize target network\n",
        "    self.Optimizer = optim.Adam(self.q_net.parameters(),lr=learning_rate ) \n",
        "    self.buffer = replaybuffer(buffer_size)  #intilize repaly buffer\n",
        "    self.criterion = nn.MSELoss()\n",
        "\n",
        "  def __select_action(self,state,episode): #selects action according to exploration-exploitation startegy\n",
        "    eps = self.eps_end + (self.eps_start-self.eps_end)*math.exp(-1.*episode/self.eps_decay) #expoential epsilon decay\n",
        "    state = torch.FloatTensor(state)\n",
        "    if random.random() > eps:   \n",
        "      with torch.no_grad():\n",
        "        return torch.argmax(self.q_net(state)).item() \n",
        "    else :\n",
        "      return torch.tensor([self.env.action_space.sample()]).item()\n",
        "  \n",
        "  def __learn(self,batch,gamma): #update network\n",
        "    self.q_net.train()  \n",
        "    rewards = batch['rewards'].reshape(len(batch['rewards']),1)  #Reshaping to 2D tensor\n",
        "    max_next_state = torch.max(self.target_net(batch['next_states'].float()).detach(),dim=1,keepdim=True)[0]\n",
        "    dones = (1-(batch['dones']).reshape(len(batch['dones']),1).to(torch.int8)) #batch['dones'] is boolen tensor, converting this to 2D int tensor\n",
        "    next_q_values = rewards + gamma*max_next_state*dones\n",
        "    actions = batch['actions'].reshape(len(batch['actions']),1).to(torch.int64) #converting to 2D tensor with dtype int64 because gather wants input as int64\n",
        "    current_q_values = self.q_net(batch['states'].float()).gather(1,actions) #gather returns elements at indexes specified in action\n",
        "    loss = self.criterion(current_q_values,next_q_values) #calculating loss\n",
        "    self.Optimizer.zero_grad() \n",
        "    loss.backward()\n",
        "    self.Optimizer.step()\n",
        "\n",
        "  def train(self,episodes,print_every=200,gamma=0.99,batch_size=128,target_update=10):  #To train network\n",
        "    for i in range(1,episodes+1):\n",
        "      state = self.env.reset()   #resetting environment at starting of each episode\n",
        "      done,total_reward,steps = False,0,0 \n",
        "      while not done :\n",
        "        action = self.__select_action(state,i) \n",
        "        next_state, reward,done,_ = self.env.step(action) \n",
        "        steps+=1\n",
        "        if done and steps<200:  #if episode terminates before 200 steps\n",
        "          reward = -1.0         #this will held network understand when it is taking wrong step\n",
        "        self.buffer.add_to_buffer(state,action,reward,next_state,done) #storing transition tuple in buffer\n",
        "        state = next_state\n",
        "        total_reward+=reward\n",
        "      if len(self.buffer)>batch_size:  \n",
        "        self.__learn(self.buffer.get_batch(batch_size),gamma) #updating network\n",
        "      if not i%target_update:  #update target network\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict()) \n",
        "      if not i%print_every :  \n",
        "        if i==episodes:\n",
        "          print(\"Finished training ðŸ™Œ\")\n",
        "        else:\n",
        "          print(f\"Training...........Finished episode:{i} Total reward earned for this episode:{total_reward}\")\n",
        "          total_reward = 0\n",
        "  \n",
        "  def play(self): \n",
        "    state = self.env.reset()\n",
        "    total_reward,done,steps = 0, False,0\n",
        "    while True:    \n",
        "      img = Image.fromarray( self.env.render(mode='rgb_array') )\n",
        "      ipyd.clear_output(wait=True)\n",
        "      ipyd.display(img)\n",
        "      sleep(0.1) \n",
        "      if done:\n",
        "        break\n",
        "      action = torch.argmax(self.q_net(torch.tensor(state).float())).item()\n",
        "      next_state, reward , done , _ = self.env.step(action)\n",
        "      total_reward+=reward\n",
        "      steps+=1 \n",
        "      state = next_state\n",
        "    print(\"Total reward:\",total_reward)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "wlKJ5xCYC6k7",
        "outputId": "1c659923-d28c-463f-a7db-b544bc8be452"
      },
      "source": [
        "agent = DQN_Agent()\n",
        "agent.play()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=600x400 at 0x7F4B53315710>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward: 12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpP-H1v-EwNN",
        "outputId": "f254fe3f-6aca-4837-dde5-c87e40976d16"
      },
      "source": [
        "agent.train(episodes=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...........Finished episode:200 Total reward earned for this episode:8.0\n",
            "Training...........Finished episode:400 Total reward earned for this episode:16.0\n",
            "Training...........Finished episode:600 Total reward earned for this episode:47.0\n",
            "Training...........Finished episode:800 Total reward earned for this episode:36.0\n",
            "Training...........Finished episode:1000 Total reward earned for this episode:26.0\n",
            "Training...........Finished episode:1200 Total reward earned for this episode:64.0\n",
            "Training...........Finished episode:1400 Total reward earned for this episode:42.0\n",
            "Training...........Finished episode:1600 Total reward earned for this episode:35.0\n",
            "Training...........Finished episode:1800 Total reward earned for this episode:56.0\n",
            "Training...........Finished episode:2000 Total reward earned for this episode:48.0\n",
            "Training...........Finished episode:2200 Total reward earned for this episode:200.0\n",
            "Training...........Finished episode:2400 Total reward earned for this episode:200.0\n",
            "Training...........Finished episode:2600 Total reward earned for this episode:192.0\n",
            "Training...........Finished episode:2800 Total reward earned for this episode:39.0\n",
            "Training...........Finished episode:3000 Total reward earned for this episode:179.0\n",
            "Training...........Finished episode:3200 Total reward earned for this episode:200.0\n",
            "Training...........Finished episode:3400 Total reward earned for this episode:200.0\n",
            "Training...........Finished episode:3600 Total reward earned for this episode:99.0\n",
            "Training...........Finished episode:3800 Total reward earned for this episode:31.0\n",
            "Training...........Finished episode:4000 Total reward earned for this episode:65.0\n",
            "Training...........Finished episode:4200 Total reward earned for this episode:200.0\n",
            "Training...........Finished episode:4400 Total reward earned for this episode:141.0\n",
            "Training...........Finished episode:4600 Total reward earned for this episode:190.0\n",
            "Training...........Finished episode:4800 Total reward earned for this episode:166.0\n",
            "Finished training ðŸ™Œ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "Fu1TnAubGxBl",
        "outputId": "cbcea1cd-8329-4c26-8d35-3cb4d9444d9c"
      },
      "source": [
        "agent.play()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=600x400 at 0x7F4B52197490>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Total reward: 200.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}