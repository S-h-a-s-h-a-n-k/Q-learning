{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Frozenlake8x8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd-Dq1V-KbHa"
      },
      "source": [
        "import gym\n",
        "import numpy as np "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJxxM00mMjIq"
      },
      "source": [
        "#Approach\n",
        "In frozen lake the agent gets 1.0 reward for reaching the goal and 0.0 reward in all other cases involving falling in hole. So we will try to redefine reward for falling in hole. This is a basic approach more better methods can be thought off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKQ7rmNtKKKX",
        "outputId": "69397b60-9f07-4467-d520-197db7618d8c"
      },
      "source": [
        "fl=gym.make('FrozenLake8x8-v0').env\n",
        "q_table=np.zeros((fl.observation_space.n,fl.action_space.n))\n",
        "episodes=5000\n",
        "alpha=0.6\n",
        "gamma=0.9\n",
        "epsilon=1.0\n",
        "for i in range (1,episodes+1):\n",
        "  state=fl.reset()\n",
        "  epochs=0\n",
        "  reward=0\n",
        "  done=False\n",
        "  while not done :\n",
        "    if np.random.rand()<epsilon :\n",
        "      action = fl.action_space.sample()\n",
        "    else :\n",
        "      action = np.argmax(q_table[state])\n",
        "    state_2,r,done,_=fl.step(action)\n",
        "    if done and not r :\n",
        "      r=-100\n",
        "    elif r==1.0 :\n",
        "      r=20\n",
        "    q_table[state,action]+=alpha*(r+gamma*np.max(q_table[state_2])-q_table[state,action])\n",
        "    state=state_2\n",
        "    r=int((r+100)/120)*1.0\n",
        "    reward+=r\n",
        "    epochs+=1\n",
        "  epsilon=0.01+0.99*np.exp(-0.001*episodes)\n",
        "  if not i%200 :\n",
        "    print(f\"Finished {i}th episode with reward:{reward} and Timesteps taken: {epochs}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished 200th episode with reward:0.0 and Timesteps taken: 4176\n",
            "Finished 400th episode with reward:0.0 and Timesteps taken: 579\n",
            "Finished 600th episode with reward:0.0 and Timesteps taken: 486\n",
            "Finished 800th episode with reward:0.0 and Timesteps taken: 1025\n",
            "Finished 1000th episode with reward:0.0 and Timesteps taken: 247\n",
            "Finished 1200th episode with reward:0.0 and Timesteps taken: 4197\n",
            "Finished 1400th episode with reward:1.0 and Timesteps taken: 74\n",
            "Finished 1600th episode with reward:1.0 and Timesteps taken: 62\n",
            "Finished 1800th episode with reward:1.0 and Timesteps taken: 71\n",
            "Finished 2000th episode with reward:1.0 and Timesteps taken: 106\n",
            "Finished 2200th episode with reward:0.0 and Timesteps taken: 48\n",
            "Finished 2400th episode with reward:1.0 and Timesteps taken: 338\n",
            "Finished 2600th episode with reward:0.0 and Timesteps taken: 28\n",
            "Finished 2800th episode with reward:1.0 and Timesteps taken: 99\n",
            "Finished 3000th episode with reward:1.0 and Timesteps taken: 143\n",
            "Finished 3200th episode with reward:1.0 and Timesteps taken: 141\n",
            "Finished 3400th episode with reward:1.0 and Timesteps taken: 100\n",
            "Finished 3600th episode with reward:1.0 and Timesteps taken: 32\n",
            "Finished 3800th episode with reward:1.0 and Timesteps taken: 941\n",
            "Finished 4000th episode with reward:1.0 and Timesteps taken: 107\n",
            "Finished 4200th episode with reward:1.0 and Timesteps taken: 162\n",
            "Finished 4400th episode with reward:1.0 and Timesteps taken: 292\n",
            "Finished 4600th episode with reward:1.0 and Timesteps taken: 103\n",
            "Finished 4800th episode with reward:1.0 and Timesteps taken: 176\n",
            "Finished 5000th episode with reward:1.0 and Timesteps taken: 129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgvdeXldKZwC",
        "outputId": "41576713-25e1-4ab8-b37f-c2f23f3b5e33"
      },
      "source": [
        "#Illustration of FrozenLake8x8-v0\n",
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "state_2=fl.reset()\n",
        "done=False\n",
        "reward=0\n",
        "fl.render()\n",
        "while not done :\n",
        "    clear_output(wait=True) \n",
        "    action=np.argmax(q_table[state])\n",
        "    state,r,done,_=fl.step(action)\n",
        "    reward+=r\n",
        "    fl.render()\n",
        "    sleep(0.1)\n",
        "print(\"Reward:\",reward)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "Reward: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}